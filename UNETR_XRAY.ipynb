{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qFQvAqFSD1vC",
        "UMlufNYGFWse",
        "Ae1Og13ED8WQ",
        "05g2tMlzUNeY"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and preprocessing"
      ],
      "metadata": {
        "id": "qFQvAqFSD1vC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "151EJAUM95PJ"
      },
      "outputs": [],
      "source": [
        "# !pip install monai\n",
        "# !pip install tensorboard-plugin-3d\n",
        "# !pip install pynvml\n",
        "# !pip install einops\n",
        "\n",
        "# !unzip /content/drive/MyDrive/X-Ray_segmentation.zip\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import einops\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from monai.networks.nets import SwinUNETR\n",
        "from monai.metrics import compute_dice, compute_iou, CumulativeAverage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/covid_segmentation')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "logger = logging.getLogger('Xray_logger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "file_log = logging.FileHandler('xray.log')\n",
        "file_log.setLevel(logging.DEBUG)\n",
        "logger.addHandler(file_log)\n",
        "logger.propagate = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_addresses():\n",
        "  ''' collect paths to images from given folders '''\n",
        "  \n",
        "  folder = '/content/X-Ray_segmentation/Covid-xray/'\n",
        "\n",
        "  files_dict = {'Train': [], 'Test': [], 'Val': []}\n",
        "\n",
        "  for eval_method in files_dict:\n",
        "    link = folder + eval_method + '/'\n",
        "    image_files = os.listdir(link + 'images/')\n",
        "\n",
        "    for image_file in image_files:\n",
        "      image = link + 'images/' + image_file\n",
        "      mask = link + 'masks/' + image_file\n",
        "      files_dict[eval_method].append((image, mask))\n",
        "  \n",
        "  return files_dict"
      ],
      "metadata": {
        "id": "CsogNqis-Dtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XRay:\n",
        "  ''' load and preprocess X-Ray images '''\n",
        "\n",
        "  def __init__(self, image_path, mask_path):\n",
        "    self.image_path = image_path\n",
        "    self.mask_path = mask_path\n",
        "  \n",
        "  def get_images(self):\n",
        "    image, mask = read_image(self.image_path), read_image(self.mask_path)\n",
        "    image, mask = self.to_standart_format(image, mask)\n",
        "\n",
        "    return (image, mask)\n",
        "  \n",
        "  def to_standart_format(self, image, mask):\n",
        "    ''' transform to standart image format '''\n",
        "    \n",
        "    if image.shape[1] > 256:\n",
        "      resize = torchvision.transforms.Resize(256)\n",
        "      image, mask = resize(image), resize(mask)\n",
        "\n",
        "    elif image.shape[1] < 256:\n",
        "      pad = torchvision.transforms.Pad(256-image.shape[1])\n",
        "      image, mask = pad(image), pad(mask)\n",
        "    \n",
        "    image = image/255\n",
        "    mask = mask/255\n",
        "\n",
        "    if image.shape[0] > 1:\n",
        "      image = image[0]\n",
        "    \n",
        "    return (image, mask)"
      ],
      "metadata": {
        "id": "R192mmAHAnJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XRayDataset(Dataset):\n",
        "  ''' create Dataset object from one of the sets\n",
        "      params:\n",
        "        data_dict: dictionary of filenames for each set\n",
        "        mode: which set to use\n",
        "        transforms: list of transforms to apply\n",
        "  '''\n",
        "\n",
        "  def __init__(self, data_dict, mode='Train', transforms=None):\n",
        "    self.data_addresses = data_dict[mode]\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_addresses)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    image_path, mask_path = self.data_addresses[idx]\n",
        "    image, mask = XRay(image_path, mask_path).get_images()\n",
        "\n",
        "    if self.transforms:\n",
        "      randtransf = transforms.RandomChoice(self.transforms) # pick random augmentation method\n",
        "      # apply augmentation to image and mask simultaneously\n",
        "      combined = torch.cat((image.unsqueeze(0), mask.unsqueeze(0)), 0)\n",
        "      image, mask = randtransf(combined)\n",
        "\n",
        "    return (image, mask)"
      ],
      "metadata": {
        "id": "0mqMbnnHC9jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "UMlufNYGFWse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluation:\n",
        "  ''' class for training and evaluation of a given model\n",
        "      params:\n",
        "        model: an object of a model to evaluate\n",
        "        loader_dict: dictionary of running modes and their DataLoader objects\n",
        "  '''\n",
        "\n",
        "  def __init__(self, model, loader_dict):\n",
        "    self.model = model\n",
        "    self.loader_dict = loader_dict\n",
        "\n",
        "    weight = torch.tensor([3], device=device) # increase weight of positive instances\n",
        "    self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
        "\n",
        "  \n",
        "  def train(self, epochs=1, batch_size=1):\n",
        "    ''' run training loop '''\n",
        "\n",
        "    self.model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    for epoch in range(epochs):\n",
        "      loss_cumul = CumulativeAverage()\n",
        "      iou_cumul = CumulativeAverage()\n",
        "      dice_cumul = CumulativeAverage()\n",
        "      for batch, (xray, mask) in enumerate(self.loader_dict['train']):\n",
        "        xray = xray.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        model_out = self.model(xray)\n",
        "        loss = self.loss_fn(model_out, mask)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        model_out = nn.Sigmoid()(model_out)\n",
        "        dice, iou = self.compute_metrics(model_out, mask)\n",
        "        loss_cumul.append(loss, count=batch_size)\n",
        "        iou_cumul.append(iou)\n",
        "        dice_cumul.append(dice)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          writer.add_scalar('Loss/train', loss, batch)\n",
        "          writer.add_scalar('IOU/train', iou, batch)\n",
        "          writer.add_images('model_out/train', (model_out > 0.5), global_step=batch)\n",
        "          writer.add_images('mask/train', mask, global_step=batch)\n",
        "          logger.debug(f'DEBUG| location: ModelEvalutation.train | epoch: {epoch}, batch: {batch}, loss: {loss.item()}, IOU: {iou}, Dice: {dice}')\n",
        "      \n",
        "      loss_avg = loss_cumul.aggregate()\n",
        "      iou_avg = iou_cumul.aggregate()\n",
        "      dice_avg = dice_cumul.aggregate()\n",
        "      writer.add_scalar('Loss_AVG/train', loss_avg, epoch)\n",
        "      writer.add_scalar('IOU_AVG/train', iou_avg, epoch)\n",
        "      logger.debug(f'DEBUG| location: ModelEvalutation.train | loss_avg: {loss_avg}, iou_avg: {iou_avg}, , dice_avg: {dice_avg}')\n",
        "      self.scheduler.step(loss)\n",
        "\n",
        "    writer.flush()\n",
        "  \n",
        "\n",
        "  def evaluate(self, mode='val', batch_size=1):\n",
        "    ''' run evaluation loop '''\n",
        "\n",
        "    self.model.eval()\n",
        "    loss_cumul = CumulativeAverage()\n",
        "    iou_cumul = CumulativeAverage()\n",
        "    dice_cumul = CumulativeAverage()\n",
        "    with torch.no_grad():\n",
        "      for batch, (xray, mask) in enumerate(self.loader_dict[mode]):\n",
        "        xray = xray.to(device)\n",
        "        mask = mask.to(device)\n",
        "        model_out = self.model(xray)\n",
        "        loss = self.loss_fn(model_out, mask)\n",
        "\n",
        "        model_out = nn.Sigmoid()(model_out)\n",
        "        dice, iou = self.compute_metrics(model_out, mask)\n",
        "        loss_cumul.append(loss, count=batch_size)\n",
        "        iou_cumul.append(iou)\n",
        "        dice_cumul.append(dice)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          writer.add_scalar(f'Loss/{mode}', loss, batch)\n",
        "          writer.add_scalar(f'IOU/{mode}', iou, batch)\n",
        "          writer.add_images(f'model_out/{mode}', (model_out > 0.5), global_step=batch)\n",
        "          writer.add_images(f'mask/{mode}', mask, global_step=batch)\n",
        "      \n",
        "    loss_avg = loss_cumul.aggregate()\n",
        "    iou_avg = iou_cumul.aggregate()\n",
        "    dice_avg = dice_cumul.aggregate() \n",
        "            \n",
        "    print(f'loss_avg: {loss_avg}, iou_avg: {iou_avg}, dice_avg: {dice_avg}')\n",
        "\n",
        "    writer.flush()\n",
        "  \n",
        "  def to_monai_form(self, y_pred, y):\n",
        "    ''' transform xray and mask to binary tensor '''\n",
        "    \n",
        "    y_pred = y_pred > 0.5\n",
        "    y = y > 0.5\n",
        "    return (y_pred, y)\n",
        "\n",
        "  def compute_metrics(self, y_pred, y):\n",
        "    ''' compute Dice coefficient and IOU '''\n",
        "\n",
        "    y_pred, y = self.to_monai_form(y_pred, y)\n",
        "    Dice = compute_dice(y_pred, y, ignore_empty=False).mean()\n",
        "    IOU = compute_iou(y_pred, y, ignore_empty=False).mean()\n",
        "    return (float(Dice), float(IOU))"
      ],
      "metadata": {
        "id": "iV3ss1TJFZy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "Ae1Og13ED8WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SwinUNETR(in_channels=1, out_channels=1, img_size=(256,256), drop_rate=0.5, spatial_dims=2, use_checkpoint=True).to(device)"
      ],
      "metadata": {
        "id": "SC8ypa_nD71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = [transforms.RandomRotation(180), transforms.RandomAffine(180),\n",
        "                 transforms.ElasticTransform(), transforms.RandomHorizontalFlip(p=0.6),\n",
        "                 transforms.RandomVerticalFlip(p=0.6), transforms.GaussianBlur(3)]"
      ],
      "metadata": {
        "id": "opkjRKJqEEHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = collect_addresses()\n",
        "\n",
        "trainset = XRayDataset(data_dict, 'Train', transforms=augmentations)\n",
        "valset = XRayDataset(data_dict, 'Val')\n",
        "testset = XRayDataset(data_dict, 'Test')\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
        "                         shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True,\n",
        "                       num_workers=num_workers, pin_memory=True)\n",
        "testloader = DataLoader(testset, batch_size=batch_size,\n",
        "                        num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "loader_dict = {'train': trainloader, 'val': valloader, 'test': testloader}"
      ],
      "metadata": {
        "id": "CYQpB-6hEGFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained state\n",
        "# model.load_state_dict(torch.load('/content/xray_model_3.pt'))\n",
        "\n",
        "evaluate = ModelEvaluation(model, loader_dict)\n",
        "evaluate.train(epochs=20, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "b09oSb9-GZyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate.evaluate('val', batch_size)"
      ],
      "metadata": {
        "id": "PJoKhBw_PqMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b46722-b8ce-4d5c-a53a-7aaa533065b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_avg: 0.17495934665203094, iou_avg: 0.5416697263717651, dice_avg: 0.6001932621002197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate.evaluate('test', batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lga3ieLRwIbt",
        "outputId": "af1c772c-c22b-4f38-fb36-5dceaa20d680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_avg: 0.17938169836997986, iou_avg: 0.5601587295532227, dice_avg: 0.6206120848655701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), '/content/xray_model.pt')"
      ],
      "metadata": {
        "id": "qNdQ9sXYRQbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "05g2tMlzUNeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = SwinUNETR(in_channels=1, out_channels=1, img_size=(256,256), drop_rate=0.5, spatial_dims=2, use_checkpoint=True).to(device)\n",
        "loaded_model.load_state_dict(torch.load('/content/xray_model.pt'))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "id": "AEbZnEj8UZF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_preprocessing(image_path):\n",
        "  image = read_image(image_path)\n",
        "  \n",
        "  if image.shape[1] > 256:\n",
        "    resize = torchvision.transforms.Resize(256)\n",
        "    image = resize(image)\n",
        "\n",
        "  elif image.shape[1] < 256:\n",
        "    pad = torchvision.transforms.Pad(256-image.shape[1])\n",
        "    image = pad(image)\n",
        "  \n",
        "  image = image/255\n",
        "\n",
        "  if image.shape[0] > 1:\n",
        "    image = image[0]\n",
        "  \n",
        "  return image"
      ],
      "metadata": {
        "id": "JMyCyNvGU_7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/X-Ray_segmentation/Covid-xray/Test/images/covid_1774.png'\n",
        "image = image_preprocessing(image_path)\n",
        "model_out = loaded_model(image.unsqueeze(0).to(device))\n",
        "model_out = model_out.detach().cpu()\n",
        "plt.imshow(model_out[0].permute(1,2,0) > 0.5)"
      ],
      "metadata": {
        "id": "bB0DgJpWVmf1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}