{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8OlrEDds6Xf"
      },
      "source": [
        "## Data Loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "URjymX51R2_r"
      },
      "outputs": [],
      "source": [
        "# !pip install SimpleITK\n",
        "# !pip install torchio\n",
        "# !pip install monai\n",
        "# !pip install einops\n",
        "# !pip install tensorboard-plugin-3d\n",
        "# !pip install pynvml\n",
        "\n",
        "# !unzip /content/drive/MyDrive/COVID-19-20_v2.zip\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchio as tio\n",
        "import matplotlib.pyplot as plt\n",
        "import SimpleITK as sitk\n",
        "import functools\n",
        "import logging\n",
        "import einops\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from monai.networks.nets import VNet, UNETR, SwinUNETR\n",
        "from monai.metrics import compute_generalized_dice, compute_average_surface_distance\n",
        "from monai.metrics import compute_surface_dice, compute_roc_auc, compute_iou\n",
        "from monai.metrics import compute_hausdorff_distance, CumulativeAverage\n",
        "from monai.visualize.img2tensorboard import plot_2d_or_3d_image\n",
        "from monai.losses import GeneralizedDiceLoss, DiceLoss, DiceCELoss\n",
        "from functools import cached_property\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/covid_segmentation')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "logger = logging.getLogger('CT_logger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "file_log = logging.FileHandler('ct.log')\n",
        "file_log.setLevel(logging.DEBUG)\n",
        "logger.addHandler(file_log)\n",
        "logger.propagate = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wDx4thT7C3iv"
      },
      "outputs": [],
      "source": [
        "xlsx = pd.ExcelFile('/content/COVID-19-20_TrainValidation.xlsx')\n",
        "df_train = pd.read_excel(xlsx, 'Train set')\n",
        "df_test = pd.read_excel(xlsx, 'Validation set')\n",
        "\n",
        "val_split = int(len(df_train)*0.8)\n",
        "\n",
        "train_namelist = list(df_train['FILENAME'])[:val_split]\n",
        "val_namelist = list(df_train['FILENAME'])[val_split:]\n",
        "test_namelist = list(df_test['FILENAME'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RtHjSAKdCrbc"
      },
      "outputs": [],
      "source": [
        "class CT:\n",
        "  def __init__(self, CT_ID, folder_path):\n",
        "    self.CT_ID = CT_ID\n",
        "    self.folder_path = folder_path\n",
        "  \n",
        "  @cached_property\n",
        "  def volume(self):\n",
        "    try: #change that\n",
        "      ct = sitk.ReadImage(self.folder_path + self.CT_ID + '.nii')\n",
        "    except Exception:\n",
        "      ct = sitk.ReadImage(self.folder_path + self.CT_ID + '_ct.nii')\n",
        "    mask = sitk.ReadImage(self.folder_path + self.CT_ID + '_seg.nii')\n",
        "\n",
        "    ct_np = sitk.GetArrayFromImage(ct)\n",
        "    mask_np = sitk.GetArrayFromImage(mask)\n",
        "\n",
        "    ct_np = np.clip(ct_np, -1000, 1000)\n",
        "    mask_np = np.clip(mask_np, -1000, 1000)\n",
        "\n",
        "    ct_tr = torch.from_numpy(ct_np).to(dtype=torch.float32).permute(1,2,0)\n",
        "    mask_tr = torch.from_numpy(mask_np).to(dtype=torch.float32).permute(1,2,0)\n",
        "\n",
        "    ct_tr = torch.nn.functional.interpolate(ct_tr, 32)\n",
        "    mask_tr = torch.nn.functional.interpolate(mask_tr, 32)\n",
        "\n",
        "    return (ct_tr, mask_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZgFo-4nISgrV"
      },
      "outputs": [],
      "source": [
        "def augment(ct, mask, aug_type): # think if it should stay a stand alone function\n",
        "  aug_dict = {'flip': tio.RandomFlip(), \n",
        "              'ED': tio.RandomElasticDeformation(),\n",
        "              'affine': tio.RandomAffine(),\n",
        "              'anistropy': tio.RandomAnisotropy(),\n",
        "              'noise': tio.RandomNoise(),\n",
        "              'blur': tio.RandomBlur(), \n",
        "              'swap': tio.RandomSwap()}\n",
        "  \n",
        "  combined = torch.cat((ct.unsqueeze(0), mask.unsqueeze(0)), 0)\n",
        "\n",
        "  ct, mask = aug_dict[aug_type](combined)\n",
        "\n",
        "  return ct, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "moe_zO__Y2Fc"
      },
      "outputs": [],
      "source": [
        "class CTDataset(Dataset):\n",
        "  def __init__(self, namelist, folder_path, augmentation_list=[]):\n",
        "    self.folder_path = folder_path\n",
        "    namelist_l = len(namelist) # change variable names\n",
        "    self.namelist = zip(namelist, [None]*namelist_l)\n",
        "    self.namelist = list(self.namelist)\n",
        "    \n",
        "    if augmentation_list:\n",
        "      for aug_type in augmentation_list:\n",
        "        self.namelist += list(zip(namelist, [aug_type]*namelist_l))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.namelist)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    ct_id, aug_type = self.namelist[idx]\n",
        "    ct, mask = CT(ct_id, self.folder_path).volume\n",
        "\n",
        "    if aug_type:\n",
        "      ct, mask = augment(ct, mask, aug_type)\n",
        "\n",
        "    return ct, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaINJzUPbx4a"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y8jv0Io3kTev"
      },
      "outputs": [],
      "source": [
        "class ModelEvaluation:\n",
        "  def __init__(self, model, loader_dict):\n",
        "    self.model = model\n",
        "    self.loader_dict = loader_dict\n",
        "\n",
        "    self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "\n",
        "  \n",
        "  def train(self, epochs=1, batch_size=1):\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # run_avg = CumulativeAverage()\n",
        "    \n",
        "    # torch.autograd.set_detect_anomaly(True)\n",
        "    self.model.train()\n",
        "    for epoch in range(epochs):\n",
        "      for batch, (ct, mask) in enumerate(self.loader_dict['train']):\n",
        "        ct = ct.unsqueeze(1).to(device)\n",
        "        mask = mask.unsqueeze(1).to(device)\n",
        "        model_out = self.model(ct)\n",
        "        loss = self.loss_fn(model_out, mask)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # run_avg.append(loss, count=batch_size)\n",
        "\n",
        "        if batch == 0: # temporary\n",
        "          logger.debug(f'DEBUG| location: ModelEvalutation.train | min: \\\n",
        "          {float(torch.min(model_out))}, max: {float(torch.max(model_out))}')\n",
        "\n",
        "\n",
        "        model_out = model_out.detach().cpu()\n",
        "        mask = mask.detach().cpu()\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        model_out, mask = sigmoid(model_out), sigmoid(mask)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          metrics = Metrics(model_out, mask).compute()\n",
        "          writer.add_scalar('Loss/train', loss, batch)\n",
        "          writer.add_scalar('IOU/train', metrics['IOU'], batch)\n",
        "          plot_2d_or_3d_image((model_out > 0.7), step=batch, writer=writer, tag='model_out') # !\n",
        "          plot_2d_or_3d_image(mask, step=batch, writer=writer, tag='mask')\n",
        "          logger.debug(f'DEBUG| location: ModelEvalutation.train | epoch: {epoch}, batch: {batch}, loss: {loss.item()}')\n",
        "          logger.debug(f'DEBUG| location: ModelEvalutation.train | {str(metrics)}')\n",
        "      \n",
        "\n",
        "    # avg_loss = run_avg.aggregate()\n",
        "    # logger.debug(f'DEBUG| location: ModelEvalutation.train | avg_loss: {avg_loss}')\n",
        "\n",
        "    writer.add_graph(self.model, ct)\n",
        "    writer.flush()\n",
        "  \n",
        "  def evaluate(self, mode='val', batch_size=1):\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    run_avg = CumulativeAverage()\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch, (ct, mask) in enumerate(self.loader_dict[mode]):\n",
        "        ct = ct.unsqueeze(1).to(device, dtype=torch.float32)\n",
        "        mask = mask.unsqueeze(1).to(device, dtype=torch.float32)\n",
        "        model_out = self.model(ct)\n",
        "        loss = self.loss_fn(model_out, mask)\n",
        "        run_avg.append(loss, count=batch_size)\n",
        "\n",
        "        if batch % 10 == 0: \n",
        "          metrics = Metrics(model_out, mask).compute()\n",
        "          writer.add_scalar(f'Loss/{mode}', loss, batch)\n",
        "          writer.add_scalar(f'F1/{mode}', metrics['F1'], batch)\n",
        "          plot_2d_or_3d_image((model_out > 0.5), step=batch, writer=writer, tag='model_out')\n",
        "          plot_2d_or_3d_image(mask, step=batch, writer=writer, tag='mask')\n",
        "    \n",
        "    avg_loss = run_avg.aggregate()\n",
        "    print('avg_loss:', avg_loss)\n",
        "\n",
        "    writer.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2EdGQbteFMTW"
      },
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "  def __init__(self, y_pred, y):\n",
        "    self.y_pred, self.y = self.to_monai_shape(y_pred, y)\n",
        "  \n",
        "  def compute(self):\n",
        "    return self.segmentation_metrics()\n",
        "  \n",
        "  def to_monai_shape(self, y_pred, y):\n",
        "    y[0][0][0][0][0] = 1\n",
        "    y_pred[0][0][0][0][0] = 1\n",
        "\n",
        "    y_pred = y_pred.permute(0,1,3,4,2)\n",
        "    y_pred = y_pred > 0.5\n",
        "    y = y.permute(0,1,3,4,2)\n",
        "    y = y > 0.5\n",
        "    return (y_pred, y)\n",
        "\n",
        "  def segmentation_metrics(self):\n",
        "    GDL = compute_generalized_dice(self.y_pred, self.y, False)\n",
        "    ASD = compute_average_surface_distance(self.y_pred, self.y)\n",
        "    IOU = compute_iou(self.y_pred, self.y)\n",
        "    Hausdorff = compute_hausdorff_distance(self.y_pred, self.y)\n",
        "\n",
        "    return {'IOU': float(IOU), 'GDL': float(GDL), 'ASD': float(ASD), 'Hausdorff': float(Hausdorff)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMPS1zlV_2-9"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e1ULBJs-AAz0"
      },
      "outputs": [],
      "source": [
        "model = VNet(in_channels=1, out_channels=1).to(device, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pF0cIj7K_64Z"
      },
      "outputs": [],
      "source": [
        "trainset = CTDataset(train_namelist, '/content/Train/', augmentation_list=['flip', 'blur', 'noise', 'swap'])\n",
        "\n",
        "valset = CTDataset(val_namelist, '/content/Train/')\n",
        "testset = CTDataset(test_namelist, '/content/Validation/')\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=batch_size)\n",
        "\n",
        "loader_dict = {'train': trainloader, 'val': valloader, 'test': testloader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBT5-tL9EweB"
      },
      "outputs": [],
      "source": [
        "evaluate = ModelEvaluation(model, loader_dict)\n",
        "evaluate.train(batch_size=batch_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B8OlrEDds6Xf"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}