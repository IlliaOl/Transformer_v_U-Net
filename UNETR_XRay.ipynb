{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qFQvAqFSD1vC",
        "UMlufNYGFWse",
        "NCOW5bakYZBB"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and preprocessing"
      ],
      "metadata": {
        "id": "qFQvAqFSD1vC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "151EJAUM95PJ"
      },
      "outputs": [],
      "source": [
        "# !pip install monai\n",
        "# !pip install tensorboard-plugin-3d\n",
        "# !pip install pynvml\n",
        "# !pip install einops\n",
        "\n",
        "# !unzip /content/drive/MyDrive/X-Ray_segmentation.zip\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import einops\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from monai.networks.nets import UNet, BasicUNet, FlexibleUNet\n",
        "from monai.networks.nets import VNet, UNETR, SwinUNETR\n",
        "from monai.metrics import compute_dice, compute_iou, CumulativeAverage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/swinunetr')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "logger = logging.getLogger('Xray_logger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "file_log = logging.FileHandler('xray.log')\n",
        "file_log.setLevel(logging.DEBUG)\n",
        "logger.addHandler(file_log)\n",
        "logger.propagate = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_addresses():\n",
        "  ''' collect paths to images from given folders '''\n",
        "  \n",
        "  folder = '/content/X-Ray_segmentation/Covid-xray/'\n",
        "\n",
        "  files_dict = {'Train': [], 'Test': [], 'Val': []}\n",
        "\n",
        "  for eval_method in files_dict:\n",
        "    link = folder + eval_method + '/'\n",
        "    image_files = os.listdir(link + 'images/')\n",
        "\n",
        "    for image_file in image_files:\n",
        "      image = link + 'images/' + image_file\n",
        "      mask = link + 'masks/' + image_file\n",
        "      files_dict[eval_method].append((image, mask))\n",
        "  \n",
        "  return files_dict"
      ],
      "metadata": {
        "id": "CsogNqis-Dtw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XRay:\n",
        "  ''' load and preprocess X-Ray images '''\n",
        "\n",
        "  def __init__(self, image_path, mask_path):\n",
        "    self.image_path = image_path\n",
        "    self.mask_path = mask_path\n",
        "  \n",
        "  def get_images(self):\n",
        "    image, mask = read_image(self.image_path), read_image(self.mask_path)\n",
        "    image, mask = self.to_standart_format(image, mask)\n",
        "\n",
        "    return (image, mask)\n",
        "  \n",
        "  def to_standart_format(self, image, mask):\n",
        "    ''' transform to standart image format '''\n",
        "    \n",
        "    if image.shape[1] > 256:\n",
        "      resize = torchvision.transforms.Resize(256)\n",
        "      image, mask = resize(image), resize(mask)\n",
        "\n",
        "    elif image.shape[1] < 256:\n",
        "      pad = torchvision.transforms.Pad(256-image.shape[1])\n",
        "      image, mask = pad(image), pad(mask)\n",
        "    \n",
        "    image = image/255\n",
        "    mask = mask/255\n",
        "\n",
        "    if image.shape[0] > 1:\n",
        "      image = image[0]\n",
        "    \n",
        "    return (image, mask)"
      ],
      "metadata": {
        "id": "R192mmAHAnJn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XRayDataset(Dataset):\n",
        "  ''' create Dataset object from one of the sets\n",
        "      params:\n",
        "        data_dict: dictionary of filenames for each set\n",
        "        mode: which set to use\n",
        "        transforms: list of transforms to apply\n",
        "  '''\n",
        "\n",
        "  def __init__(self, data_dict, mode='Train', transforms=None):\n",
        "    self.data_addresses = data_dict[mode]\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_addresses)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    image_path, mask_path = self.data_addresses[idx]\n",
        "    image, mask = XRay(image_path, mask_path).get_images()\n",
        "\n",
        "    if self.transforms:\n",
        "      randtransf = transforms.RandomChoice(self.transforms) # pick random augmentation method\n",
        "      # apply augmentation to image and mask simultaneously\n",
        "      combined = torch.cat((image.unsqueeze(0), mask.unsqueeze(0)), 0)\n",
        "      image, mask = randtransf(combined)\n",
        "\n",
        "    return (image, mask)"
      ],
      "metadata": {
        "id": "0mqMbnnHC9jR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "UMlufNYGFWse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluation:\n",
        "  ''' class for training and evaluation of a given model\n",
        "      params:\n",
        "        model: an object of a model to evaluate\n",
        "        loader_dict: dictionary of running modes and their DataLoader objects\n",
        "  '''\n",
        "\n",
        "  def __init__(self, model, loader_dict):\n",
        "    self.model = model\n",
        "    self.loader_dict = loader_dict\n",
        "\n",
        "    weight = torch.tensor([4], device=device) # increase weight of positive instances\n",
        "    self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
        "\n",
        "  \n",
        "  def train(self, epochs=1, batch_size=1):\n",
        "    ''' run training loop '''\n",
        "\n",
        "    self.model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    for epoch in range(epochs):\n",
        "      loss_cumul = CumulativeAverage()\n",
        "      iou_cumul = CumulativeAverage()\n",
        "      dice_cumul = CumulativeAverage()\n",
        "      recall_cumul = CumulativeAverage()\n",
        "      precision_cumul = CumulativeAverage()\n",
        "      f1_cumul = CumulativeAverage()\n",
        "      for batch, (xray, mask) in enumerate(self.loader_dict['train']):\n",
        "        xray = xray.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        model_out = self.model(xray)\n",
        "        loss = self.loss_fn(model_out, mask)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        model_out = nn.Sigmoid()(model_out)\n",
        "        dice, iou, recall, precision, f1 = self.compute_metrics(model_out, mask)\n",
        "        loss_cumul.append(loss, count=batch_size)\n",
        "\n",
        "        iou_cumul.append(iou)\n",
        "        dice_cumul.append(dice)\n",
        "        recall_cumul.append(recall)\n",
        "        precision_cumul.append(precision)\n",
        "        f1_cumul.append(f1)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          writer.add_images('model_out/train', (model_out > 0.5), global_step=batch)\n",
        "          writer.add_images('mask/train', mask, global_step=batch)\n",
        "          logger.debug(f'DEBUG| location: ModelEvalutation.train | epoch: {epoch}, batch: {batch}, loss: {loss.item()}, IOU: {iou}, Dice: {dice}')\n",
        "      \n",
        "      loss_avg = loss_cumul.aggregate()\n",
        "      iou_avg = iou_cumul.aggregate()\n",
        "      dice_avg = dice_cumul.aggregate()\n",
        "      recall_avg = recall_cumul.aggregate()\n",
        "      precision_avg = precision_cumul.aggregate()\n",
        "      f1_avg = f1_cumul.aggregate()\n",
        "\n",
        "      writer.add_scalar('Loss_AVG/train', loss_avg, epoch)\n",
        "      writer.add_scalar('IOU_AVG/train', iou_avg, epoch)\n",
        "      writer.add_scalar('Dice_AVG/train', dice_avg, epoch)\n",
        "      writer.add_scalar('Recall_AVG/train', recall_avg, epoch)\n",
        "      writer.add_scalar('Precision_AVG/train', precision_avg, epoch)\n",
        "      writer.add_scalar('F1_AVG/train', f1_avg, epoch)\n",
        "  \n",
        "      logger.debug(f'DEBUG| location: ModelEvalutation.train | loss_avg: {loss_avg}, iou_avg: {iou_avg}, , dice_avg: {dice_avg}')\n",
        "      self.scheduler.step(loss)\n",
        "\n",
        "    writer.flush()\n",
        "  \n",
        "\n",
        "  def evaluate(self, mode='val', batch_size=1):\n",
        "    ''' run evaluation loop '''\n",
        "\n",
        "    self.model.eval()\n",
        "    loss_cumul = CumulativeAverage()\n",
        "    iou_cumul = CumulativeAverage()\n",
        "    dice_cumul = CumulativeAverage()\n",
        "    recall_cumul = CumulativeAverage()\n",
        "    precision_cumul = CumulativeAverage()\n",
        "    f1_cumul = CumulativeAverage()\n",
        "    with torch.no_grad():\n",
        "      for batch, (xray, mask) in enumerate(self.loader_dict[mode]):\n",
        "        xray = xray.to(device)\n",
        "        mask = mask.to(device)\n",
        "        model_out = self.model(xray)\n",
        "        loss = self.loss_fn(model_out, mask)\n",
        "\n",
        "        model_out = nn.Sigmoid()(model_out)\n",
        "        dice, iou, recall, precision, f1 = self.compute_metrics(model_out, mask)\n",
        "        loss_cumul.append(loss, count=batch_size)\n",
        "\n",
        "        iou_cumul.append(iou)\n",
        "        dice_cumul.append(dice)\n",
        "        recall_cumul.append(recall)\n",
        "        precision_cumul.append(precision)\n",
        "        f1_cumul.append(f1)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          writer.add_images(f'model_out/{mode}', (model_out > 0.5), global_step=batch)\n",
        "          writer.add_images(f'mask/{mode}', mask, global_step=batch)\n",
        "      \n",
        "    loss_avg = loss_cumul.aggregate()\n",
        "    iou_avg = iou_cumul.aggregate()\n",
        "    dice_avg = dice_cumul.aggregate()\n",
        "    recall_avg = recall_cumul.aggregate()\n",
        "    precision_avg = precision_cumul.aggregate()\n",
        "    f1_avg = f1_cumul.aggregate()\n",
        "            \n",
        "    print(f'loss_avg: {loss_avg}, iou_avg: {iou_avg}, dice_avg: {dice_avg}')\n",
        "    print(f'recall_avg: {recall_avg}, precision_avg: {precision_avg}, f1_avg: {f1_avg}')\n",
        "\n",
        "    writer.flush()\n",
        "  \n",
        "  def to_monai_form(self, y_pred, y):\n",
        "    ''' transform xray and mask to binary tensor '''\n",
        "    \n",
        "    y_pred = y_pred > 0.5\n",
        "    y = y > 0.5\n",
        "    return (y_pred, y)\n",
        "\n",
        "  def compute_metrics(self, y_pred, y):\n",
        "    ''' compute Monai metrics '''\n",
        "\n",
        "    y_pred, y = self.to_monai_form(y_pred, y)\n",
        "    Dice = compute_dice(y_pred, y, ignore_empty=False).mean()\n",
        "    IOU = compute_iou(y_pred, y, ignore_empty=False).mean()\n",
        "\n",
        "    recall, precision, f1 = self.additional_metrics(y_pred, y)\n",
        "    \n",
        "    metrics = [float(Dice), float(IOU), float(recall),\n",
        "               float(precision), float(f1)]\n",
        "\n",
        "    return metrics\n",
        "  \n",
        "  def additional_metrics(self, y_pred, y):\n",
        "    ''' compute additional metrics '''\n",
        "\n",
        "    inter = (y_pred * y).sum(dim=[1,2,3])\n",
        "    \n",
        "    recall = (inter + 1)/(y.sum(dim=[1,2,3]) + 1)\n",
        "    precision = (inter + 1)/(y_pred.sum(dim=[1,2,3]) + 1)\n",
        "    f1 = 2*((precision*recall)/(precision+recall))\n",
        "\n",
        "    return recall.mean(), precision.mean(), f1.mean()"
      ],
      "metadata": {
        "id": "iV3ss1TJFZy6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "Ae1Og13ED8WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SwinUNETR(in_channels=1, out_channels=1, img_size=(256,256), drop_rate=0.5, spatial_dims=2, use_checkpoint=True).to(device)\n",
        "# model = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=(4, 8, 16), strides=(2, 2)).to(device)\n",
        "# model = VNet(in_channels=1, out_channels=1, spatial_dims=2).to(device)\n",
        "# model = BasicUNet(spatial_dims=2, in_channels=1, out_channels=1, dropout=0.5).to(device)\n",
        "# model = UNETR(in_channels=1, out_channels=1, img_size=(256,256), dropout_rate=0.5, spatial_dims=2).to(device)\n",
        "# model = FlexibleUNet(in_channels=1, out_channels=1, backbone='efficientnet-b0', spatial_dims=2).to(device)"
      ],
      "metadata": {
        "id": "SC8ypa_nD71S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = [transforms.RandomRotation(180), transforms.RandomAffine(180),\n",
        "                 transforms.ElasticTransform(), transforms.RandomHorizontalFlip(p=0.6),\n",
        "                 transforms.RandomVerticalFlip(p=0.6), transforms.GaussianBlur(3)]"
      ],
      "metadata": {
        "id": "opkjRKJqEEHj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = collect_addresses()\n",
        "\n",
        "trainset = XRayDataset(data_dict, 'Train')\n",
        "valset = XRayDataset(data_dict, 'Val')\n",
        "testset = XRayDataset(data_dict, 'Test')\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
        "                         shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True,\n",
        "                       num_workers=num_workers, pin_memory=True)\n",
        "testloader = DataLoader(testset, batch_size=batch_size,\n",
        "                        num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "loader_dict = {'train': trainloader, 'val': valloader, 'test': testloader}"
      ],
      "metadata": {
        "id": "CYQpB-6hEGFJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained state\n",
        "# model.load_state_dict(torch.load('/content/xray_model_3.pt'))\n",
        "\n",
        "evaluate = ModelEvaluation(model, loader_dict)\n",
        "evaluate.train(epochs=15, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "b09oSb9-GZyN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate.evaluate('val', batch_size)"
      ],
      "metadata": {
        "id": "PJoKhBw_PqMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810632a7-88f8-47e5-a495-24dd1b381b30"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_avg: 0.23560521006584167, iou_avg: 0.5917662382125854, dice_avg: 0.648518443107605\n",
            "recall_avg: 0.8311728835105896, precision_avg: 0.7285675406455994, f1_avg: 0.6581042408943176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate.evaluate('test', batch_size)"
      ],
      "metadata": {
        "id": "Lga3ieLRwIbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c07670-20c1-48f1-ed3a-d9ea0fdc684c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_avg: 0.25243592262268066, iou_avg: 0.600612461566925, dice_avg: 0.6573601961135864\n",
            "recall_avg: 0.8254415392875671, precision_avg: 0.7375897765159607, f1_avg: 0.665016770362854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), '/content/xray_model.pt')"
      ],
      "metadata": {
        "id": "qNdQ9sXYRQbN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "05g2tMlzUNeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = SwinUNETR(in_channels=1, out_channels=1, img_size=(256,256), spatial_dims=2).to(device)\n",
        "loaded_model.load_state_dict(torch.load('/content/xray_model_3.pt'))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "id": "AEbZnEj8UZF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "89bdf5ab-5ab7-4728-e10f-55ea0066f544"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9459f72a6a7c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwinUNETR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/xray_model_3.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/xray_model_3.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_preprocessing(image_path):\n",
        "  image = read_image(image_path)\n",
        "  \n",
        "  if image.shape[1] > 256:\n",
        "    resize = torchvision.transforms.Resize(256)\n",
        "    image = resize(image)\n",
        "\n",
        "  elif image.shape[1] < 256:\n",
        "    pad = torchvision.transforms.Pad(256-image.shape[1])\n",
        "    image = pad(image)\n",
        "  \n",
        "  image = image/255\n",
        "\n",
        "  if image.shape[0] > 1:\n",
        "    image = image[0]\n",
        "  \n",
        "  return image"
      ],
      "metadata": {
        "id": "JMyCyNvGU_7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/X-Ray_segmentation/Covid-xray/Test/images/covid_1702.png'\n",
        "image = image_preprocessing(image_path)\n",
        "model_out = loaded_model(image.unsqueeze(0).to(device))\n",
        "model_out = model_out.detach().cpu()"
      ],
      "metadata": {
        "id": "Xe25YZjs0aSh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}